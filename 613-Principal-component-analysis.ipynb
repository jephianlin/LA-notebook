{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主成份分析\n",
    "\n",
    "Principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Creative Commons License](https://i.creativecommons.org/l/by/4.0/88x31.png)  \n",
    "This work by Jephian Lin is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\trans}{^\\top}\n",
    "\\newcommand{\\adj}{^{\\rm adj}}\n",
    "\\newcommand{\\cof}{^{\\rm cof}}\n",
    "\\newcommand{\\inp}[2]{\\left\\langle#1,#2\\right\\rangle}\n",
    "\\newcommand{\\dunion}{\\mathbin{\\dot\\cup}}\n",
    "\\newcommand{\\bzero}{\\mathbf{0}}\n",
    "\\newcommand{\\bone}{\\mathbf{1}}\n",
    "\\newcommand{\\ba}{\\mathbf{a}}\n",
    "\\newcommand{\\bb}{\\mathbf{b}}\n",
    "\\newcommand{\\bc}{\\mathbf{c}}\n",
    "\\newcommand{\\bd}{\\mathbf{d}}\n",
    "\\newcommand{\\be}{\\mathbf{e}}\n",
    "\\newcommand{\\bh}{\\mathbf{h}}\n",
    "\\newcommand{\\bp}{\\mathbf{p}}\n",
    "\\newcommand{\\bq}{\\mathbf{q}}\n",
    "\\newcommand{\\br}{\\mathbf{r}}\n",
    "\\newcommand{\\bx}{\\mathbf{x}}\n",
    "\\newcommand{\\by}{\\mathbf{y}}\n",
    "\\newcommand{\\bz}{\\mathbf{z}}\n",
    "\\newcommand{\\bu}{\\mathbf{u}}\n",
    "\\newcommand{\\bv}{\\mathbf{v}}\n",
    "\\newcommand{\\bw}{\\mathbf{w}}\n",
    "\\newcommand{\\tr}{\\operatorname{tr}}\n",
    "\\newcommand{\\nul}{\\operatorname{null}}\n",
    "\\newcommand{\\rank}{\\operatorname{rank}}\n",
    "%\\newcommand{\\ker}{\\operatorname{ker}}\n",
    "\\newcommand{\\range}{\\operatorname{range}}\n",
    "\\newcommand{\\Col}{\\operatorname{Col}}\n",
    "\\newcommand{\\Row}{\\operatorname{Row}}\n",
    "\\newcommand{\\spec}{\\operatorname{spec}}\n",
    "\\newcommand{\\vspan}{\\operatorname{span}}\n",
    "\\newcommand{\\Vol}{\\operatorname{Vol}}\n",
    "\\newcommand{\\sgn}{\\operatorname{sgn}}\n",
    "\\newcommand{\\idmap}{\\operatorname{id}}\n",
    "\\newcommand{\\am}{\\operatorname{am}}\n",
    "\\newcommand{\\gm}{\\operatorname{gm}}\n",
    "\\newcommand{\\mult}{\\operatorname{mult}}\n",
    "\\newcommand{\\iner}{\\operatorname{iner}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lingeo import random_int_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X$ be an $N\\times d$ data matrix whose the sample vectors are centered at $\\bzero\\in\\mathbb{R}^d$.  \n",
    "Let $C = \\frac{1}{N-1}X\\trans X$ be the covariance matrix.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\bv$ be a unit vector in $\\mathbb{R}^d$.  \n",
    "One may project all data points onto the direction of $\\bv$.  \n",
    "Indeed, the projected data is $\\bu = X\\bv$.  \n",
    "Thus, the mean of $\\bu$ is $\\inp{\\bu}{\\bone} = \\bone\\trans X\\bu = \\bzero\\trans\\bu = 0$,  \n",
    "and the variance of $\\bu$ is $\\frac{1}{N-1}\\inp{\\bu}{\\bu} = \\frac{1}{N-1}\\bu\\trans X\\trans X\\bu = \\bv\\trans C\\bv$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is natural to consider the optimization problem:  \n",
    "\n",
    "maximize $\\bv\\trans C\\bv$,  \n",
    "subject to $\\|\\bv\\| = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\lambda_1\\geq \\cdots \\geq \\lambda_d$ be the eigenvalues of $C$ and $\\{\\bv_1,\\ldots,\\bv_d\\}$ the corresponding orthonormal eigenbasis.  \n",
    "According to the Rayleigh quotient theorem, the maximum is achieved by $\\lambda_1$ when $\\bv = \\bv_1$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, $\\bv_1$ is called the first **principal component** of the data,  \n",
    "and one may continue to take $\\bv_2,\\bv_3,\\ldots$ as the next principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Algorithm (principal component analysis)\n",
    "\n",
    "Input: a data represented by an $N\\times d$ matrix $X$ and the desired number $k$ of principal components  \n",
    "Output: the principal components represented by the column vectors of a matrix $P$\n",
    "\n",
    "1. Let $J$ be the $N\\times N$ all-ones matrix.  \n",
    "Define $X_0 = (I - \\frac{1}{n}J)X$.  \n",
    "2. Calculate the covariance matrix $C= \\frac{1}{N-1}X_0\\trans X_0$.\n",
    "3. Find a diagonal matrix $D$, whose diagonal entries are $\\lambda_1 \\geq \\cdots \\geq \\lambda_d$, and an orthogonal matrix $Q$ such that $C = QDQ\\trans$.\n",
    "3. Let $P$ be the $d\\times k$ matrix obtained by the first $k$ columns of $Q$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few points to emphasize:\n",
    "\n",
    "- The $Q$ matrix is the same as the $V$ in the algorithm in 606.  \n",
    "- The eigenvalues $\\lambda_1 \\geq \\cdots \\geq \\lambda_d$ are the variances when the data are projected onto the columns of $Q$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side stories\n",
    "\n",
    "- explained variance ratio\n",
    "- `scipy.linalg.eigh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 1\n",
    "\n",
    "執行以下程式碼。  \n",
    "\n",
    "<!-- eng start -->\n",
    "Run the code below.  \n",
    "<!-- eng end -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### code\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)\n",
    "print_ans = False\n",
    "\n",
    "mu = np.random.randint(-3,4, (2,))\n",
    "cov = np.array([[1,1.9],\n",
    "                [1.9,4]])\n",
    "X = np.random.multivariate_normal(mu, cov, (20,))\n",
    "mu = X.mean(axis=0)\n",
    "X0 = X - mu\n",
    "# u,s,vh = np.linalg.svd(X0)\n",
    "C = (1 / (19)) * X0.T.dot(X0)\n",
    "vals,vecs = np.linalg.eigh(C)\n",
    "vals = vals[::-1]\n",
    "vecs = vecs[:,::-1]\n",
    "P = vecs[:,:1]\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.scatter(*X.T)\n",
    "plt.scatter(*mu, c=\"red\")\n",
    "plt.arrow(*mu, *(vecs[:,0]), head_width=0.3, color=\"red\")\n",
    "\n",
    "xs = X0.dot(P)\n",
    "ys = np.zeros_like(xs)\n",
    "plt.scatter(xs, ys)\n",
    "\n",
    "pretty_print(LatexExpr(\"Q =\"))\n",
    "print(vecs)\n",
    "\n",
    "if print_ans:\n",
    "    print(\"red point: center\")\n",
    "    print(\"red arrow: first principal component\")\n",
    "    print(\"orange points: projection of blue points onto the red arrow\")\n",
    "    print(\"red arrow = first column of Q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 1(a)\n",
    "\n",
    "若藍點為給定的資料。  \n",
    "改變不同的 `seed`，說明紅點、紅箭頭、橘點的意思。  \n",
    "\n",
    "<!-- eng start -->\n",
    "Let the blue points be the dataset.  Try different `seed` and explore the meaning of the red point, the red arrow, and the orange point.   \n",
    "<!-- eng end -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 1(b)\n",
    "\n",
    "令 $X_0$ 的列向量為將藍點資料置中過後的資料點、  \n",
    "而 $C = \\frac{1}{N-1}X_0\\trans X_0$ 為其共變異數矩陣。  \n",
    "若 $C = QDQ\\trans$ 為 $C$ 的譜分解，其中 $D$ 的對角線項由大到小排列。  \n",
    "說明紅箭頭與 $Q$ 的關係。  \n",
    "\n",
    "<!-- eng start -->\n",
    "Let $X_0$ be the matrix whose rows record the data from the blue points, centered at their mean.  Let $C = \\frac{1}{N-1}X_0\\trans X_0$ be the covariance matrix.  Suppose $C = QDQ\\trans$ is the spectral decomposition of $C$ such that the diagonal entries of $D$ is arranged from the largest to the smallest.  Explain the relation between the red arrow and $Q$.  \n",
    "<!-- eng end -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 2\n",
    "\n",
    "令 $X$ 是已置中的 $N\\times d$ 資料矩陣、  \n",
    "而 $C = \\frac{1}{N-1}X_0\\trans X_0$ 為其共變異數矩陣。  \n",
    "令 $\\bv$ 為一 $\\mathbb{R}^d$ 中的單位向量。  \n",
    "說明 $\\bu = X\\bv$ 為將 $X$ 的所有樣本點投影在 $\\bv$ 方向的值、  \n",
    "並說明 $\\bu$ 的變異數為 $\\bv\\trans C\\bv$。  \n",
    "\n",
    "<!-- eng start -->\n",
    "Let $X$ be a $N\\times d$ data matrix whose rows are centered at their mean.  Let $C = \\frac{1}{N-1}X_0\\trans X_0$ be the covariance matrix.  Fix a unit vector $\\bv$ in $\\mathbb{R}^d$.  Show that $\\bu = X\\bv$ records the length of data points in $X$ projected on $\\bv$ and show that the variance of $\\bu$ is $\\bv\\trans C\\bv$.  \n",
    "<!-- eng end -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3\n",
    "\n",
    "執行以下程式碼。  \n",
    "\n",
    "<!-- eng start -->\n",
    "Run the code below.  \n",
    "<!-- eng end -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.randn(3,4)\n",
    "u,s,vh = np.linalg.svd(X) \n",
    "vals,vecs = np.linalg.eigh(X.T.dot(X))\n",
    "vals = vals[::-1]\n",
    "vecs = vecs[:,::-1]\n",
    "print(\"V =\")\n",
    "print(vh.T)\n",
    "print(\"Q =\")\n",
    "print(vecs)\n",
    "print(\"singular values:\", s)\n",
    "print(\"eigenvalues:\", vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3(a)\n",
    "\n",
    "說明為什麼印出來的兩個矩陣會一樣。  \n",
    "（除了有的行會有正負號的差別。）  \n",
    "\n",
    "<!-- eng start -->\n",
    "Explain why the two output matrices are the same (except that some of the columns are negated).  \n",
    "<!-- eng end -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3(b)\n",
    "\n",
    "說明印出來的奇異值和特徵值有什麼關係。  \n",
    "\n",
    "<!-- eng start -->\n",
    "Explain the relation between the singular values and the eigenvalues in the output.  \n",
    "<!-- eng end -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3(c)\n",
    "\n",
    "解釋  \n",
    "\n",
    "```python\n",
    "vals = vals[::-1]\n",
    "vecs = vecs[:,::-1]\n",
    "```\n",
    "\n",
    "的用處。  \n",
    "\n",
    "<!-- eng start -->\n",
    "Explain the meaning of  \n",
    "\n",
    "```python\n",
    "vals = vals[::-1]\n",
    "vecs = vecs[:,::-1]\n",
    "```\n",
    "<!-- eng end -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 4\n",
    "\n",
    "有時候我們只須要最大的幾個特徵值。  \n",
    "所以如果把所有的特徵值都算出來是不必要的浪費時間。  \n",
    "查找資源如何利用 SciPy 中的 `linalg.eigh` 找最大的幾個特徵值。  \n",
    "\n",
    "<!-- eng start -->\n",
    "Sometimes we only need the first few largest eigenvalues, and it is a waste of time to compute all eigenvalues.  Search online and find out how to find the largest eigenvalues through `linalg.eigh` in SciPy.  \n",
    "<!-- eng end -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 5\n",
    "\n",
    "給定 $k$ 時，  \n",
    "\n",
    "$$\n",
    "    p = \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{i=1}^d \\lambda_i}\n",
    "$$\n",
    "\n",
    "稱作**變異數解釋率（explained variance ratio）**。  \n",
    "當解釋率夠高的時候，表示投影後的資料足以代表原資料的重要特徵。\n",
    "\n",
    "觀察下圖，其橫軸為 $k$，縱軸為解釋率。  \n",
    "判斷 $k$ 應該要取多少，以及為什麼。  \n",
    "\n",
    "<!-- eng start -->\n",
    "For a given $k$,  \n",
    "\n",
    "$$\n",
    "    p = \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{i=1}^d \\lambda_i}\n",
    "$$\n",
    "\n",
    "is known as the **explained variance ratio** .  When this rate is high enough, it means the projected data is representative enough for the original data.  \n",
    "\n",
    "Observe the figure below, where the $x$-axis represents $k$ and the $y$-axis represents the explained variance ratio.  Determine which $k$ is preferred and give your reasons.   \n",
    "<!-- eng end -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.genfromtxt('hidden_text.csv', delimiter=',')\n",
    "print(\"shape of X =\", X.shape)\n",
    "X = X - X.mean(axis=0)\n",
    "u,s,vh = np.linalg.svd(X)\n",
    "vals = s**2\n",
    "\n",
    "plt.plot(np.arange(1,101), vals.cumsum() / vals.sum())\n",
    "plt.gca().set_xlim(0,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 6\n",
    "\n",
    "找一個共變異數矩陣 `cov` 使得 `np.random.multivariate_normal` 產出來的資料大約是一個楕圓形，且：  \n",
    "\n",
    "- 長軸指向 $45^\\circ$ 角、短軸指向 $135^\\circ$ 角；\n",
    "- 長軸是大約是短軸的 $2$ 倍。  \n",
    "\n",
    "<!-- eng start -->\n",
    "Find a covariance matrix `cov` such that the data generated by `np.random.multivariate_normal` is roughly an ellipse such that:  \n",
    "\n",
    "- the major axis is on $45^\\circ$, while the minor axis is on $135^\\circ$;\n",
    "- the major axis is about twice the length of the minor axis.   \n",
    "<!-- eng end -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mu = np.array([0,0])\n",
    "cov = np.array([\n",
    "    [1,0],\n",
    "    [0,1]\n",
    "])\n",
    "X = np.random.multivariate_normal(mu, cov, (1000,))\n",
    "plt.axis('equal')\n",
    "plt.scatter(*X.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 9.6",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
